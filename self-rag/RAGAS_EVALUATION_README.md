# RAGAS Evaluation for Self-RAG Experiments

This document explains how to use RAGAS with DeepSeek-R1 API to evaluate the Self-RAG experiment results.

## Overview

The evaluation script (`evaluate_with_ragas.py`) integrates RAGAS (RAG Assessment) framework with DeepSeek-R1 API to automatically evaluate the quality of responses generated by the Self-RAG model across three experiments:

1. **exp1_popqa**: PopQA longtail questions
2. **exp2_arc**: ARC Challenge questions
3. **exp3_health**: Health claims verification

## Setup

### 1. Install Dependencies

```bash
# Install RAGAS
cd ragas
pip install -e .

# Or install from PyPI
pip install ragas

# Install required dependencies
pip install openai datasets
```

### 2. Set DeepSeek API Key

```bash
export DEEPSEEK_API_KEY='your-deepseek-api-key-here'
```

You can get an API key from [DeepSeek Platform](https://platform.deepseek.com/).

## Usage

### Run Full Evaluation

Evaluate all three experiments:

```bash
cd /data/self-rag
python evaluate_with_ragas.py
```

This will:
- Load results from `retrieval_lm/exp1`, `retrieval_lm/exp2`, and `retrieval_lm/exp3_debug`
- Match predictions with ground truth from evaluation data
- Use DeepSeek-R1 to evaluate quality with RAGAS metrics
- Save results to `ragas_results/` directory

### Run Quick Test

For testing with a smaller sample:

```bash
python evaluate_with_ragas_simple.py
```

This will evaluate only the first 10 samples from each experiment.

## RAGAS Metrics

The script evaluates the following metrics:

### 1. **Answer Relevancy**
- Measures how relevant the generated answer is to the question
- Score: 0-1 (higher is better)

### 2. **Answer Correctness**
- Compares the generated answer with ground truth
- Combines semantic similarity and factual overlap
- Score: 0-1 (higher is better)
- Only computed when ground truth is available

### 3. **Faithfulness**
- Measures if the answer is faithful to the retrieved context
- Checks for hallucinations
- Score: 0-1 (higher is better)
- Only computed when contexts are available

### 4. **Context Precision**
- Measures if relevant context chunks are ranked higher
- Score: 0-1 (higher is better)
- Only computed when contexts and ground truth are available

### 5. **Context Recall**
- Measures if all relevant information is retrieved
- Score: 0-1 (higher is better)
- Only computed when contexts and ground truth are available

## Output Format

Results are saved in JSON format:

```json
{
  "experiment": "exp1_popqa",
  "num_samples": 1000,
  "metrics": {
    "answer_relevancy": 0.85,
    "answer_correctness": 0.72,
    "faithfulness": 0.91,
    "context_precision": 0.68,
    "context_recall": 0.74
  }
}
```

A summary of all experiments is saved to `ragas_results/summary.json`.

## Project Structure

```
/data/self-rag/
├── evaluate_with_ragas.py          # Main evaluation script
├── evaluate_with_ragas_simple.py   # Simple test script
├── RAGAS_EVALUATION_README.md      # This file
├── retrieval_lm/
│   ├── exp1                        # PopQA results
│   ├── exp2                        # ARC Challenge results
│   └── exp3_debug                  # Health claims results
├── eval_data/
│   ├── popqa_longtail_w_gs.jsonl
│   ├── arc_challenge_processed.jsonl
│   └── health_claims_processed.jsonl
└── ragas_results/                  # Output directory
    ├── exp1_popqa_ragas_eval.json
    ├── exp2_arc_ragas_eval.json
    ├── exp3_health_ragas_eval.json
    └── summary.json
```

## Customization

### Adjusting Sample Size

To evaluate only a subset of samples, modify the `parse_self_rag_output` function:

```python
samples = parse_self_rag_output(data, eval_data_path)
samples = samples[:100]  # Evaluate only first 100 samples
```

### Adding More Metrics

You can add additional RAGAS metrics:

```python
from ragas.metrics import ContextRelevancy, SemanticSimilarity

metrics = [
    AnswerRelevancy(),
    AnswerCorrectness(),
    ContextRelevancy(),
    SemanticSimilarity(),
]
```

### Using Different LLM

To use a different LLM (e.g., OpenAI GPT-4):

```python
from openai import OpenAI

client = OpenAI(api_key="your-openai-key")
llm = llm_factory("gpt-4", client=client)
```

## Troubleshooting

### API Key Not Found

```
Error: DEEPSEEK_API_KEY environment variable not set
```

**Solution**: Set the environment variable:
```bash
export DEEPSEEK_API_KEY='your-key'
```

### Out of Memory

If you encounter memory issues:
- Reduce batch size
- Evaluate experiments one at a time
- Use the simple script with smaller samples

### Rate Limiting

If you hit API rate limits:
- Add delays between API calls
- Reduce the number of samples
- Use a higher-tier API plan

## References

- [RAGAS Documentation](https://docs.ragas.io/)
- [DeepSeek API](https://platform.deepseek.com/api-docs)
- [Self-RAG Paper](https://arxiv.org/abs/2310.11511)

## Notes

- The evaluation uses DeepSeek-R1 (deepseek-reasoner) model which is designed for reasoning tasks
- Evaluation time depends on the number of samples and API response time
- Results may vary slightly between runs due to LLM variability
- Make sure you have sufficient API credits before running full evaluation
