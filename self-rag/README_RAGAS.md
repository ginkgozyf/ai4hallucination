# Self-RAG RAGAS è¯„ä¼°ç³»ç»Ÿ / Self-RAG RAGAS Evaluation System

## âœ… å®ŒæˆçŠ¶æ€ / Completion Status

å·²å®Œæˆ Self-RAG å®éªŒç»“æœä¸ RAGAS è¯„ä¼°æ¡†æ¶çš„é›†æˆï¼Œä½¿ç”¨ DeepSeek-R1 API è¿›è¡Œè‡ªåŠ¨åŒ–è¯„ä¼°ã€‚

**Integration of Self-RAG experiment results with RAGAS evaluation framework using DeepSeek-R1 API is complete.**

---

## ğŸ“ é¡¹ç›®æ–‡ä»¶ / Project Files

### æ ¸å¿ƒè„šæœ¬ / Core Scripts

| æ–‡ä»¶ File | è¯´æ˜ Description | æ ·æœ¬æ•° Samples |
|---|---|---|
| `evaluate_with_ragas.py` | **ä¸»è¯„ä¼°è„šæœ¬** (æ¨è)<br>Main evaluation script (recommended) | 20 per exp |
| `evaluate_with_ragas_simple.py` | å¿«é€Ÿæµ‹è¯•è„šæœ¬<br>Quick test script | 10 per exp |
| `setup_ragas_eval.sh` | è‡ªåŠ¨è®¾ç½®æ£€æŸ¥<br>Automated setup check | - |

### æ–‡æ¡£æ–‡ä»¶ / Documentation

| æ–‡ä»¶ File | è¯´æ˜ Description |
|---|---|
| `RUN_EVALUATION.md` | **â­ è¿è¡ŒæŒ‡å— (ä»è¿™é‡Œå¼€å§‹)**<br>**â­ Run guide (start here)** |
| `QUICKSTART_RAGAS.md` | å¿«é€Ÿå…¥é—¨æŒ‡å—<br>Quick start guide |
| `RAGAS_EVALUATION_README.md` | è¯¦ç»†æŠ€æœ¯æ–‡æ¡£<br>Detailed technical docs |
| `RAGAS_INTEGRATION_SUMMARY.md` | é¡¹ç›®æ¶æ„æ€»ç»“<br>Project architecture summary |
| `CHANGES.md` | ä¿®æ”¹æ—¥å¿—<br>Change log |
| `README_RAGAS.md` | æœ¬æ–‡ä»¶<br>This file |

---

## ğŸš€ å¿«é€Ÿå¼€å§‹ / Quick Start

### 1ï¸âƒ£ è®¾ç½® API å¯†é’¥

```bash
export DEEPSEEK_API_KEY='your-api-key'
```

### 2ï¸âƒ£ è¿è¡Œè¯„ä¼° (æ¨èé…ç½®)

```bash
cd /data/self-rag
python3 evaluate_with_ragas.py
```

è¿™å°†è¯„ä¼°ï¼š
- exp1_popqa: 20 samples
- exp2_arc: 20 samples  
- exp3_health: 20 samples
- **æ€»è®¡ Total: 60 samples**

### 3ï¸âƒ£ æŸ¥çœ‹ç»“æœ

```bash
cat ragas_results/summary.json
```

---

## ğŸ“Š è¯„ä¼°é…ç½® / Evaluation Configuration

### å½“å‰è®¾ç½® / Current Settings

```
æ¯ä¸ªå®éªŒæ ·æœ¬æ•° Samples per experiment: 20
è¯„ä¼°æ—¶é—´ Evaluation time:             5-10 åˆ†é’Ÿ minutes
API æˆæœ¬ API cost:                    ä½ Low
```

### è¯„ä¼°æŒ‡æ ‡ / Metrics

âœ“ Answer Relevancy (å›ç­”ç›¸å…³æ€§)
âœ“ Answer Correctness (å›ç­”æ­£ç¡®æ€§)
âœ“ Faithfulness (å¿ å®åº¦) *
âœ“ Context Precision (ä¸Šä¸‹æ–‡ç²¾ç¡®åº¦) *
âœ“ Context Recall (ä¸Šä¸‹æ–‡å¬å›ç‡) *

*ä»…åœ¨ä¸Šä¸‹æ–‡å¯ç”¨æ—¶è®¡ç®—

---

## ğŸ“ˆ ä½¿ç”¨æµç¨‹ / Workflow

```
1. è®¾ç½® API å¯†é’¥
   Set API Key
   â†“
2. è¿è¡Œè®¾ç½®æ£€æŸ¥ (å¯é€‰)
   Run setup check (optional)
   ./setup_ragas_eval.sh
   â†“
3. è¿è¡Œè¯„ä¼°
   Run evaluation
   python3 evaluate_with_ragas.py
   â†“
4. æŸ¥çœ‹ç»“æœ
   View results
   ragas_results/*.json
   â†“
5. åˆ†æå’Œæ¯”è¾ƒ
   Analyze and compare
```

---

## ğŸ¯ ä¸‰ä¸ªå®éªŒ / Three Experiments

| å®éªŒ | æ•°æ®é›† | ä»»åŠ¡ç±»å‹ | æ ·æœ¬æ•° |
|---|---|---|---|
| **exp1_popqa** | PopQA longtail | é—®ç­”<br>QA | 20 |
| **exp2_arc** | ARC Challenge | é€‰æ‹©é¢˜<br>Multiple Choice | 20 |
| **exp3_health** | Health Claims | éªŒè¯<br>Verification | 20 |

---

## ğŸ’¡ å¸¸ç”¨å‘½ä»¤ / Common Commands

### è¯„ä¼° / Evaluation

```bash
# 20 samples (æ¨è recommended)
python3 evaluate_with_ragas.py

# 10 samples (æ›´å¿« faster)
python3 evaluate_with_ragas_simple.py

# æ£€æŸ¥è®¾ç½® check setup
./setup_ragas_eval.sh
```

### æŸ¥çœ‹ç»“æœ / View Results

```bash
# å•ä¸ªå®éªŒ single experiment
cat ragas_results/exp1_popqa_ragas_eval.json

# æ€»ç»“ summary
cat ragas_results/summary.json

# æ ¼å¼åŒ–è¾“å‡º formatted output
python3 -m json.tool ragas_results/summary.json
```

### ä¿®æ”¹æ ·æœ¬æ•° / Change Sample Count

ç¼–è¾‘ `evaluate_with_ragas.py` ç¬¬263è¡Œ:

```python
MAX_SAMPLES = 20  # æ”¹ä¸ºä»»æ„æ•°å­—æˆ– None (å…¨éƒ¨)
                  # Change to any number or None (all)
```

---

## ğŸ“š è¯¦ç»†æ–‡æ¡£ / Detailed Documentation

æƒ³äº†è§£æ›´å¤šï¼ŸæŸ¥çœ‹è¿™äº›æ–‡æ¡£ï¼š

**Want to know more? Check these docs:**

1. **RUN_EVALUATION.md** - è¯¦ç»†è¿è¡ŒæŒ‡å—å’Œæ•…éšœæ’é™¤
2. **QUICKSTART_RAGAS.md** - å®‰è£…å’Œè®¾ç½®è¯´æ˜
3. **RAGAS_EVALUATION_README.md** - æŠ€æœ¯ç»†èŠ‚å’Œè‡ªå®šä¹‰
4. **CHANGES.md** - ä¿®æ”¹æ—¥å¿—

---

## ğŸ” ç»“æœç¤ºä¾‹ / Result Example

```json
{
  "experiment": "exp1_popqa",
  "num_samples": 20,
  "metrics": {
    "answer_relevancy": 0.8234,
    "answer_correctness": 0.7156
  }
}
```

---

## âš™ï¸ æŠ€æœ¯æ ˆ / Tech Stack

- **è¯„ä¼°æ¡†æ¶ Evaluation Framework**: RAGAS
- **LLM**: DeepSeek-R1 (deepseek-reasoner)
- **è¯­è¨€ Language**: Python 3.8+
- **ä¸»è¦ä¾èµ– Dependencies**: ragas, openai, datasets

---

## ğŸ†˜ éœ€è¦å¸®åŠ©? / Need Help?

### é—®é¢˜è¯Šæ–­ / Issue Diagnosis

```bash
# 1. æ£€æŸ¥è®¾ç½®
./setup_ragas_eval.sh

# 2. æŸ¥çœ‹è¯¦ç»†é”™è¯¯
python3 evaluate_with_ragas.py 2>&1 | tee error.log

# 3. éªŒè¯æ–‡ä»¶
ls -la retrieval_lm/exp* eval_data/*.jsonl
```

### å¸¸è§é—®é¢˜ / Common Issues

**Q: API å¯†é’¥æœªè®¾ç½®**
```bash
export DEEPSEEK_API_KEY='your-key'
```

**Q: åŒ…æœªå®‰è£…**
```bash
pip install ragas openai datasets
```

**Q: æ–‡ä»¶æœªæ‰¾åˆ°**
```bash
cd /data/self-rag  # ç¡®ä¿åœ¨æ­£ç¡®ç›®å½•
```

---

## ğŸ“ æ”¯æŒ / Support

- ğŸ“– æŸ¥çœ‹æ–‡æ¡£ Check documentation
- ğŸ› æŠ¥å‘Šé—®é¢˜ Report issues  
- ğŸ’¬ æå‡ºå»ºè®® Suggest improvements

---

## âœ¨ ç‰¹æ€§ / Features

âœ… è‡ªåŠ¨åŒ–è¯„ä¼° Automated evaluation
âœ… å¤šæŒ‡æ ‡æ”¯æŒ Multiple metrics
âœ… çµæ´»é…ç½® Flexible configuration
âœ… è¯¦ç»†æ–‡æ¡£ Comprehensive documentation
âœ… ä¸­è‹±åŒè¯­ Bilingual support
âœ… å¿«é€Ÿæµ‹è¯•æ¨¡å¼ Quick test mode
âœ… æ‰¹é‡è¯„ä¼° Batch evaluation
âœ… ç»“æœä¿å­˜ Result persistence

---

## ğŸ‰ å°±ç»ªè¿è¡Œ! / Ready to Run!

æ‰€æœ‰è®¾ç½®å·²å®Œæˆï¼Œä½ å¯ä»¥ç«‹å³å¼€å§‹è¯„ä¼°ï¼

**All setup is complete, you can start evaluation immediately!**

```bash
cd /data/self-rag
export DEEPSEEK_API_KEY='your-key'
python3 evaluate_with_ragas.py
```

**ç¥ä½ è¯„ä¼°é¡ºåˆ©! Good luck with your evaluation!** ğŸš€

---

**ç‰ˆæœ¬ Version**: 1.0
**æ›´æ–°æ—¥æœŸ Last Updated**: 2025-10-29
**çŠ¶æ€ Status**: âœ… ç”Ÿäº§å°±ç»ª Production Ready
