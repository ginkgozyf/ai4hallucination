import os
import json
import torch
import spacy
import matplotlib.pyplot as plt
import numpy as np
from tqdm import tqdm
from typing import List, Dict, Tuple
from selfcheckgpt.modeling_selfcheck import SelfCheckNLI
from openai import OpenAI
import pandas as pd

# åˆå§‹åŒ–OpenAIå®¢æˆ·ç«¯ï¼Œé…ç½®åƒé—®API
client = OpenAI(
    # ä»ç¯å¢ƒå˜é‡è·å–APIå¯†é’¥ï¼Œè‹¥æœªé…ç½®å¯ç›´æ¥æ›¿æ¢ä¸º"sk-xxx"æ ¼å¼çš„å¯†é’¥
    api_key="sk-aa3b65d89c824754804f4291b1540e88",
    # ç™¾ç‚¼å…¼å®¹æ¨¡å¼APIç«¯ç‚¹
    base_url="https://dashscope.aliyuncs.com/compatible-mode/v1",
)

# --------------------------
# 0. ç¯å¢ƒåˆå§‹åŒ–ä¸é…ç½®
# --------------------------
# å®‰è£…ä¾èµ–å‘½ä»¤ï¼ˆé¦–æ¬¡è¿è¡Œå‰æ‰§è¡Œï¼‰ï¼š
# pip install spacy matplotlib numpy selfcheckgpt transformers>=4.35 torch>=2.0 dashscope tqdm pandas
# python -m spacy download en_core_web_sm

# å…¨å±€é…ç½® - ä¿®å¤å­—ä½“é—®é¢˜
plt.style.use('default')  # ä½¿ç”¨é»˜è®¤æ ·å¼é¿å…å…¼å®¹æ€§é—®é¢˜
plt.rcParams['font.sans-serif'] = ['DejaVu Sans', 'Arial', 'Helvetica']  # ä½¿ç”¨æ›´é€šç”¨çš„å­—ä½“
plt.rcParams['axes.unicode_minus'] = False
plt.rcParams['font.size'] = 10
plt.rcParams['axes.titlesize'] = 12
plt.rcParams['axes.labelsize'] = 11

DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"ã€ç¯å¢ƒä¿¡æ¯ã€‘ä½¿ç”¨è®¾å¤‡ï¼š{DEVICE} | PyTorchç‰ˆæœ¬ï¼š{torch.__version__}\n")

# --------------------------
# 1. åŠ è½½MultiSpanQAæ•°æ®é›†
# --------------------------
def load_multispanqa_data(file_path: str) -> List[Dict]:
    """åŠ è½½MultiSpanQAæ•°æ®é›†ï¼Œè§£æä¸Šä¸‹æ–‡ã€é—®é¢˜å’ŒçœŸå®ç­”æ¡ˆ"""
    print("=" * 60)
    print("1. åŠ è½½MultiSpanQAæ•°æ®é›†")
    print("=" * 60)
    
    if not os.path.exists(file_path):
        raise FileNotFoundError(f"æ•°æ®é›†æ–‡ä»¶ä¸å­˜åœ¨ï¼š{file_path}")
    
    with open(file_path, "r", encoding="utf-8") as f:
        data = json.load(f)
    
    samples = []
    for item in data["data"]:
        # æ‹¼æ¥ä¸Šä¸‹æ–‡å’Œé—®é¢˜ï¼ˆåŸå§‹æ•°æ®ä¸ºå•è¯åˆ—è¡¨ï¼‰
        context = " ".join(item["context"])
        question = " ".join(item["question"])
        
        # ä»B/I/Oæ ‡ç­¾æå–çœŸå®ç­”æ¡ˆï¼ˆå¤šè·¨åº¦ï¼‰
        true_answers = []
        current_answer = []
        for word, label in zip(item["context"], item["label"]):
            if label == "B":
                if current_answer:
                    true_answers.append(" ".join(current_answer))
                current_answer = [word]
            elif label == "I":
                current_answer.append(word)
            elif label == "O" and current_answer:
                true_answers.append(" ".join(current_answer))
                current_answer = []
        if current_answer:  # å¤„ç†æœ«å°¾ç­”æ¡ˆ
            true_answers.append(" ".join(current_answer))
        
        samples.append({
            "id": item["id"],
            "context": context,
            "question": question,
            "true_answers": true_answers
        })
    
    print(f"ã€æ•°æ®é›†ä¿¡æ¯ã€‘å…±åŠ è½½{len(samples)}ä¸ªæ ·æœ¬ | ç¤ºä¾‹ID: {samples[0]['id']}")
    print(f"ã€ç¤ºä¾‹ä¸Šä¸‹æ–‡ã€‘{samples[0]['context'][:100]}...")
    print(f"ã€ç¤ºä¾‹é—®é¢˜ã€‘{samples[0]['question']}\n")
    return samples

# --------------------------
# 2. è°ƒç”¨Qwenç”Ÿæˆå›ç­”ï¼ˆå¸¦ä¸Šä¸‹æ–‡ï¼‰- æ”¯æŒå¤šä¸ªæ¨¡å‹
# --------------------------
def generate_qa_with_qwen(context: str, question: str, num_samples: int = 3, model: str = "qwen-plus") -> Tuple[str, List[str]]:
    """
    è°ƒç”¨Qwenç”Ÿæˆç›®æ ‡å›ç­”å’Œå¤šä¸ªé‡‡æ ·å›ç­”ï¼ˆåŸºäºç»™å®šä¸Šä¸‹æ–‡ï¼‰
    :return: target_answerï¼ˆç›®æ ‡å›ç­”ï¼‰, sampled_answersï¼ˆé‡‡æ ·å›ç­”åˆ—è¡¨ï¼‰
    """
    def call_qwen(prompt: str, temperature: float, model: str) -> str:
        """å•æ¬¡è°ƒç”¨Qwen APIçš„å·¥å…·å‡½æ•°"""
        try:
            completion = client.chat.completions.create(
                model=model,
                messages=[
                    {"role": "system", "content": "You are a helpful assistant."},
                    {"role": "user", "content": prompt},
                ],
                stream=True  # æµå¼è¿”å›ï¼Œé€æ®µè·å–ç»“æœ
            )
            response = ""
            for chunk in completion:
                content = chunk.choices[0].delta.content
                if content:  # ç¡®ä¿å†…å®¹ä¸ä¸ºç©º
                    response += content
                    print(content, end="", flush=True)  # å®æ—¶æ‰“å°
            print()  # è¾“å‡ºç»“æŸåæ¢è¡Œ

            return response
        except Exception as e:
            print(f"\nâŒ APIè°ƒç”¨å¤±è´¥: {str(e)}")
            return f"Error: {str(e)}"
    
    # æ„å»ºå¸¦ä¸Šä¸‹æ–‡çš„æç¤ºè¯
    base_prompt = f"ä¸Šä¸‹æ–‡ï¼š{context}\né—®é¢˜ï¼š{question}\nè¯·æ ¹æ®ä¸Šä¸‹æ–‡å›ç­”é—®é¢˜ï¼Œä¸è¦ç¼–é€ ä¿¡æ¯ã€‚"
    
    print(f"ã€ä½¿ç”¨æ¨¡å‹ã€‘{model}")
    
    # ç”Ÿæˆç›®æ ‡å›ç­”ï¼ˆä½æ¸©åº¦ç¡®ä¿ç¨³å®šæ€§ï¼‰
    target_answer = call_qwen(base_prompt, temperature=0.3, model=model)
    
    # ç”Ÿæˆé‡‡æ ·å›ç­”ï¼ˆæé«˜æ¸©åº¦å¢åŠ å¤šæ ·æ€§ï¼‰
    sampled_answers = []
    for i in range(num_samples):
        sample = call_qwen(base_prompt, temperature=0.7 + i * 0.2, model=model)
        sampled_answers.append(sample)
    
    return target_answer, sampled_answers

# --------------------------
# 3. æ–‡æœ¬é¢„å¤„ç†ä¸å¹»è§‰æ£€æµ‹
# --------------------------
def preprocess_text(text: str) -> List[str]:
    """ä½¿ç”¨spaCyåˆ†å‰²æ–‡æœ¬ä¸ºå¥å­"""
    nlp = spacy.load("en_core_web_sm")
    return [sent.text.strip() for sent in nlp(text).sents if sent.text.strip()]

def detect_hallucination(target: str, samples: List[str], context: str) -> Dict:
    """
    åŸºäºNLIçš„å¹»è§‰æ£€æµ‹ï¼š
    - ä»…ä½¿ç”¨SelfCheckGPTçš„NLIæ–¹æ³•
    """
    target_sents = preprocess_text(target)
    if not target_sents:
        return {"å¥å­": [], "NLIå¹»è§‰åˆ†æ•°": []}
    
    # åˆå§‹åŒ–NLIæ£€æµ‹å™¨
    nli = SelfCheckNLI(device=DEVICE)
    nli_scores = nli.predict(sentences=target_sents, sampled_passages=samples)
    
    # è½¬æ¢ä¸ºåˆ—è¡¨ä»¥ä¾¿JSONåºåˆ—åŒ–
    nli_scores_list = nli_scores.tolist() if hasattr(nli_scores, 'tolist') else list(nli_scores)
    
    return {
        "å¥å­": target_sents,
        "NLIå¹»è§‰åˆ†æ•°": nli_scores_list
    }

# --------------------------
# 4. å¯è§†åŒ–å‡½æ•° - ä¿®å¤é”®åé”™è¯¯
# --------------------------
def visualize_sample_results_comparison(results_dict: Dict, sample_id: str, save_dir: str):
    """å¯¹æ¯”ä¸¤ä¸ªæ¨¡å‹çš„å¯è§†åŒ–ç»“æœ - ä¿®å¤é”®åé”™è¯¯ç‰ˆæœ¬"""
    # æ£€æŸ¥æ˜¯å¦æœ‰æœ‰æ•ˆæ•°æ®
    has_valid_data = False
    for model in results_dict:
        if ("hallucination_analysis" in results_dict[model] and 
            results_dict[model]["hallucination_analysis"] and 
            "å¥å­" in results_dict[model]["hallucination_analysis"] and 
            results_dict[model]["hallucination_analysis"]["å¥å­"]):
            has_valid_data = True
            break
    
    if not has_valid_data:
        print(f"æ ·æœ¬ {sample_id} æ— æœ‰æ•ˆå¥å­å¯å¯è§†åŒ–")
        return
    
    # åˆ›å»ºå¯¹æ¯”å›¾è¡¨
    fig, axes = plt.subplots(2, 2, figsize=(16, 12))
    fig.suptitle(f'Model Comparison - Sample {sample_id}', fontsize=16, fontweight='bold')
    
    models = list(results_dict.keys())
    colors = ['#1f77b4', '#ff7f0e']  # ä¸ºä¸¤ä¸ªæ¨¡å‹åˆ†é…ä¸åŒé¢œè‰²
    
    for i, model in enumerate(models):
        if (model not in results_dict or 
            "hallucination_analysis" not in results_dict[model] or 
            not results_dict[model]["hallucination_analysis"]):
            continue
            
        result = results_dict[model]["hallucination_analysis"]
        if not result["å¥å­"]:
            continue
            
        sentences = result["å¥å­"]
        nli_scores = result["NLIå¹»è§‰åˆ†æ•°"]
        
        # å­å›¾1å’Œ2ï¼šå„æ¨¡å‹çš„NLIåˆ†æ•°æŸ±çŠ¶å›¾
        ax1 = axes[0, i]
        x_pos = np.arange(len(sentences))
        bars = ax1.bar(x_pos, nli_scores, color=colors[i], alpha=0.8, edgecolor='black', linewidth=1)
        
        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for j, (bar, score) in enumerate(zip(bars, nli_scores)):
            ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, 
                    f'{score:.3f}', ha='center', va='bottom', fontweight='bold', fontsize=9)
        
        ax1.axhline(y=0.5, color='red', linestyle='--', linewidth=2, alpha=0.7, label='Risk Threshold (0.5)')
        ax1.set_xlabel('Sentence Index', fontsize=11)
        ax1.set_ylabel('NLI Hallucination Score', fontsize=11)
        ax1.set_title(f'{model} - NLI Hallucination Detection', fontsize=13, fontweight='bold')
        ax1.set_xticks(x_pos)
        ax1.set_xticklabels([f'S{j+1}' for j in range(len(sentences))], fontsize=9)
        ax1.legend(fontsize=10)
        ax1.grid(True, alpha=0.3)
        ax1.set_ylim(0, 1)
        
        # å­å›¾3å’Œ4ï¼šå„æ¨¡å‹çš„é£é™©ç­‰çº§é¥¼å›¾
        ax2 = axes[1, i]
        high_risk = sum(1 for score in nli_scores if score > 0.5)
        low_risk = len(nli_scores) - high_risk
        
        if len(nli_scores) > 0:
            risk_data = [high_risk, low_risk]
            risk_labels = [f'High Risk\n({high_risk} sentences)', f'Low Risk\n({low_risk} sentences)']
            risk_colors_pie = ['#ff6b6b', '#51cf66']
            explode = (0.05, 0) if high_risk > 0 else (0, 0)
            
            wedges, texts, autotexts = ax2.pie(risk_data, labels=risk_labels, colors=risk_colors_pie,
                                              autopct='%1.1f%%', startangle=90, explode=explode,
                                              textprops={'fontsize': 10})
            
            for autotext in autotexts:
                autotext.set_color('white')
                autotext.set_fontweight('bold')
            
            ax2.set_title(f'{model} - Risk Distribution', fontsize=13, fontweight='bold')
    
    # è°ƒæ•´å¸ƒå±€
    plt.tight_layout(pad=3.0)
    
    # ä¿å­˜å›¾ç‰‡
    save_path = os.path.join(save_dir, f"sample_{sample_id}_comparison.png")
    plt.savefig(save_path, dpi=300, bbox_inches='tight', facecolor='white')
    plt.close()
    
    print(f"ã€Comparison Visualizationã€‘Chart saved to: {save_path}")

def create_comparison_summary_visualization(all_results: List[Dict], save_dir: str):
    """åˆ›å»ºä¸¤ä¸ªæ¨¡å‹çš„å¯¹æ¯”æ±‡æ€»å¯è§†åŒ– - ä¿®å¤é”®åé”™è¯¯ç‰ˆæœ¬"""
    if not all_results:
        return
    
    # æ”¶é›†ä¸¤ä¸ªæ¨¡å‹çš„ç»Ÿè®¡æ•°æ®
    model_stats = {}
    
    for model in ["qwen-max", "qwen-plus"]:
        all_nli_scores = []
        high_risk_counts = []
        
        for result in all_results:
            if (model in result and 
                "hallucination_analysis" in result[model] and 
                result[model]["hallucination_analysis"] and 
                "NLIå¹»è§‰åˆ†æ•°" in result[model]["hallucination_analysis"] and 
                result[model]["hallucination_analysis"]["NLIå¹»è§‰åˆ†æ•°"]):
                
                nli_scores = result[model]["hallucination_analysis"]["NLIå¹»è§‰åˆ†æ•°"]
                all_nli_scores.extend(nli_scores)
                high_risk_count = sum(1 for score in nli_scores if score > 0.5)
                high_risk_counts.append(high_risk_count)
        
        if all_nli_scores:
            model_stats[model] = {
                "all_scores": all_nli_scores,
                "high_risk_counts": high_risk_counts,
                "avg_score": np.mean(all_nli_scores),
                "high_risk_ratio": sum(1 for score in all_nli_scores if score > 0.5) / len(all_nli_scores),
                "max_score": max(all_nli_scores),
                "min_score": min(all_nli_scores)
            }
    
    if len(model_stats) < 2:
        print("Not enough models for comparison")
        return
    
    # åˆ›å»ºå¯¹æ¯”æ±‡æ€»å›¾è¡¨
    fig = plt.figure(figsize=(18, 14))
    fig.suptitle('Model Comparison - Hallucination Detection Summary', fontsize=18, fontweight='bold')
    
    # å®šä¹‰å­å›¾å¸ƒå±€
    gs = fig.add_gridspec(3, 2)
    ax1 = fig.add_subplot(gs[0, 0])  # åˆ†æ•°åˆ†å¸ƒå¯¹æ¯”
    ax2 = fig.add_subplot(gs[0, 1])  # é«˜é£é™©æ¯”ä¾‹å¯¹æ¯”
    ax3 = fig.add_subplot(gs[1, 0])  # å„æ ·æœ¬é«˜é£é™©å¥å­æ•°é‡å¯¹æ¯”
    ax4 = fig.add_subplot(gs[1, 1])  # ç»Ÿè®¡ä¿¡æ¯å¯¹æ¯”
    ax5 = fig.add_subplot(gs[2, :])  # æ¨¡å‹æ€§èƒ½é›·è¾¾å›¾
    
    models = list(model_stats.keys())
    colors = ['#1f77b4', '#ff7f0e']
    
    # å­å›¾1ï¼šåˆ†æ•°åˆ†å¸ƒå¯¹æ¯”
    for i, model in enumerate(models):
        scores = model_stats[model]["all_scores"]
        ax1.hist(scores, bins=15, alpha=0.7, color=colors[i], label=model, edgecolor='black')
    
    ax1.axvline(0.5, color='red', linestyle='--', linewidth=2, label='Risk Threshold')
    ax1.set_xlabel('NLI Hallucination Score', fontsize=11)
    ax1.set_ylabel('Frequency', fontsize=11)
    ax1.set_title('Score Distribution Comparison', fontsize=13, fontweight='bold')
    ax1.legend(fontsize=10)
    ax1.grid(True, alpha=0.3)
    
    # å­å›¾2ï¼šé«˜é£é™©æ¯”ä¾‹å¯¹æ¯”
    risk_ratios = [model_stats[model]["high_risk_ratio"] for model in models]
    x_pos = np.arange(len(models))
    bars = ax2.bar(x_pos, risk_ratios, color=colors, alpha=0.7, edgecolor='black')
    
    ax2.set_xlabel('Model', fontsize=11)
    ax2.set_ylabel('High Risk Ratio', fontsize=11)
    ax2.set_title('High Risk Ratio Comparison', fontsize=13, fontweight='bold')
    ax2.set_xticks(x_pos)
    ax2.set_xticklabels(models, fontsize=10)
    ax2.grid(True, alpha=0.3)
    
    # æ·»åŠ æ•°å€¼æ ‡ç­¾
    for bar, ratio in zip(bars, risk_ratios):
        ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, 
                f'{ratio:.1%}', ha='center', va='bottom', fontweight='bold')
    
    # å­å›¾3ï¼šå„æ ·æœ¬é«˜é£é™©å¥å­æ•°é‡å¯¹æ¯”
    sample_ids = list(range(1, len(all_results) + 1))
    width = 0.35
    x = np.arange(len(sample_ids))
    
    for i, model in enumerate(models):
        counts = model_stats[model]["high_risk_counts"][:len(sample_ids)]  # ç¡®ä¿é•¿åº¦ä¸€è‡´
        ax3.bar(x + i*width, counts, width, label=model, color=colors[i], alpha=0.7)
    
    ax3.set_xlabel('Sample Index', fontsize=11)
    ax3.set_ylabel('High Risk Sentences Count', fontsize=11)
    ax3.set_title('High Risk Sentences per Sample', fontsize=13, fontweight='bold')
    ax3.set_xticks(x + width/2)
    ax3.set_xticklabels([f'S{i+1}' for i in range(len(sample_ids))], fontsize=9, rotation=45)
    ax3.legend(fontsize=10)
    ax3.grid(True, alpha=0.3)
    
    # å­å›¾4ï¼šç»Ÿè®¡ä¿¡æ¯å¯¹æ¯”
    ax4.axis('off')
    stats_text = "Statistical Comparison:\n\n"
    for i, model in enumerate(models):
        stats = model_stats[model]
        stats_text += f"{model}:\n"
        stats_text += f"  â€¢ Avg Score: {stats['avg_score']:.3f}\n"
        stats_text += f"  â€¢ High Risk Ratio: {stats['high_risk_ratio']:.1%}\n"
        stats_text += f"  â€¢ Max Score: {stats['max_score']:.3f}\n"
        stats_text += f"  â€¢ Min Score: {stats['min_score']:.3f}\n"
        stats_text += f"  â€¢ Total Sentences: {len(stats['all_scores'])}\n\n"
    
    ax4.text(0.05, 0.95, stats_text, fontsize=11, fontfamily='monospace',
             verticalalignment='top', linespacing=1.5, transform=ax4.transAxes)
    
    # å­å›¾5ï¼šæ¨¡å‹æ€§èƒ½é›·è¾¾å›¾
    # å‡†å¤‡é›·è¾¾å›¾æ•°æ®
    categories = ['Accuracy\n(Low Score)', 'Consistency\n(Low Risk)', 'Stability\n(Min Score)', 'Reliability\n(Max Score)']
    N = len(categories)
    
    # ä¸ºæ¯ä¸ªæ¨¡å‹è®¡ç®—é›·è¾¾å›¾å€¼ï¼ˆå€¼è¶Šå¤§è¶Šå¥½ï¼‰
    values = {}
    for model in models:
        stats = model_stats[model]
        # è½¬æ¢ä¸ºæ€§èƒ½æŒ‡æ ‡ï¼ˆå€¼è¶Šå¤§è¡¨ç¤ºæ€§èƒ½è¶Šå¥½ï¼‰
        accuracy = 1 - stats['avg_score']  # å¹³å‡åˆ†æ•°è¶Šä½è¶Šå¥½
        consistency = 1 - stats['high_risk_ratio']  # é«˜é£é™©æ¯”ä¾‹è¶Šä½è¶Šå¥½
        stability = 1 - stats['min_score']  # æœ€å°åˆ†æ•°è¶Šä½è¶Šå¥½
        reliability = 1 - stats['max_score']  # æœ€å¤§åˆ†æ•°è¶Šä½è¶Šå¥½
        
        values[model] = [accuracy, consistency, stability, reliability]
    
    # ç»˜åˆ¶é›·è¾¾å›¾
    angles = np.linspace(0, 2*np.pi, N, endpoint=False).tolist()
    angles += angles[:1]  # é—­åˆå›¾å½¢
    
    for i, model in enumerate(models):
        model_values = values[model]
        model_values += model_values[:1]  # é—­åˆå›¾å½¢
        ax5.plot(angles, model_values, 'o-', linewidth=2, label=model, color=colors[i])
        ax5.fill(angles, model_values, alpha=0.25, color=colors[i])
    
    ax5.set_xticks(angles[:-1])
    ax5.set_xticklabels(categories, fontsize=10)
    ax5.set_ylim(0, 1)
    ax5.set_title('Model Performance Radar Chart\n(Higher values are better)', fontsize=13, fontweight='bold')
    ax5.legend(loc='upper right', bbox_to_anchor=(0.1, 0.1), fontsize=10)
    ax5.grid(True)
    
    # å¤§å¹…å¢åŠ å¸ƒå±€é—´è·
    plt.tight_layout(pad=4.0, h_pad=3.0, w_pad=3.0)
    
    save_path = os.path.join(save_dir, "model_comparison_summary.png")
    plt.savefig(save_path, dpi=300, bbox_inches='tight', facecolor='white')
    plt.close()
    
    print(f"ã€Model Comparison Summaryã€‘Chart saved to: {save_path}")
    
    # ä¿å­˜å¯¹æ¯”æ•°æ®åˆ°CSV
    comparison_data = []
    for model in models:
        stats = model_stats[model]
        comparison_data.append({
            'Model': model,
            'Average Score': f"{stats['avg_score']:.4f}",
            'High Risk Ratio': f"{stats['high_risk_ratio']:.2%}",
            'Max Score': f"{stats['max_score']:.4f}",
            'Min Score': f"{stats['min_score']:.4f}",
            'Total Sentences': len(stats['all_scores'])
        })
    
    df = pd.DataFrame(comparison_data)
    csv_path = os.path.join(save_dir, "model_comparison_stats.csv")
    df.to_csv(csv_path, index=False)
    print(f"ã€Comparison Statsã€‘Data saved to: {csv_path}")

# --------------------------
# 5. æ”¹è¿›çš„ä¸»å‡½æ•° - å¢å¼ºé”™è¯¯å¤„ç†
# --------------------------
def main():
    # é…ç½®å‚æ•°
    DATA_PATH = r"C:\Users\Adin\Desktop\selfcheckgpt\valid.json"  # æ•°æ®é›†è·¯å¾„ï¼ˆæ ¹æ®å®é™…ä¿®æ”¹ï¼‰
    SAVE_DIR = r"C:\Users\Adin\Desktop\selfcheckgpt\selfcheckgpt\result"  # ç»“æœä¿å­˜ç›®å½•
    NUM_SAMPLES = 3  # æ¯ä¸ªé—®é¢˜ç”Ÿæˆçš„é‡‡æ ·å›ç­”æ•°é‡
    MODELS = ["qwen-max", "qwen-plus"]  # è¦å¯¹æ¯”çš„æ¨¡å‹åˆ—è¡¨
    MAX_SAMPLES = 20  # æµ‹è¯•æ ·æœ¬æ•°é‡ï¼ˆæŒ‰éœ€è°ƒæ•´ï¼Œå»ºè®®å¼€å§‹æ—¶ç”¨å°‘é‡æ ·æœ¬æµ‹è¯•ï¼‰
    
    # åˆ›å»ºä¿å­˜ç›®å½•
    os.makedirs(SAVE_DIR, exist_ok=True)
    
    try:
        # 1. åŠ è½½æ•°æ®é›†
        samples = load_multispanqa_data(DATA_PATH)
        if MAX_SAMPLES > 0:
            samples = samples[:MAX_SAMPLES]  # é™åˆ¶æµ‹è¯•æ ·æœ¬æ•°é‡
        
        # 2. æ‰¹é‡å¤„ç†æ ·æœ¬ï¼ˆå®æ—¶ä¿å­˜ç»“æœï¼‰
        all_results = []
        processed_count = 0
        
        for i, sample in enumerate(tqdm(samples, desc="Processing Samples")):
            sample_id = sample["id"]
            context = sample["context"]
            question = sample["question"]
            true_answers = sample["true_answers"]
            
            print(f"\nã€Processing Sample {i+1}/{len(samples)}ã€‘ID: {sample_id}")
            print(f"Question: {question}")
            
            try:
                # ä¸ºå½“å‰æ ·æœ¬åˆå§‹åŒ–ç»“æœå­—å…¸
                current_result = {
                    "id": sample_id,
                    "context": context,
                    "question": question,
                    "true_answers": true_answers
                }
                
                # ä¸ºæ¯ä¸ªæ¨¡å‹ç”Ÿæˆç»“æœ
                model_results = {}
                for model in MODELS:
                    print(f"\n--- Using Model: {model} ---")
                    
                    # è°ƒç”¨Qwenç”Ÿæˆå›ç­”
                    target_answer, sampled_answers = generate_qa_with_qwen(
                        context=context,
                        question=question,
                        num_samples=NUM_SAMPLES,
                        model=model
                    )
                    
                    # æ£€æŸ¥APIè°ƒç”¨æ˜¯å¦æˆåŠŸ
                    if target_answer.startswith("Error:"):
                        print(f"âŒ Model {model} API call failed: {target_answer}")
                        continue
                    
                    # å¹»è§‰æ£€æµ‹ï¼ˆä»…ä½¿ç”¨NLIï¼‰
                    hallu_result = detect_hallucination(
                        target=target_answer,
                        samples=sampled_answers,
                        context=context
                    )
                    
                    # ä¿å­˜è¯¥æ¨¡å‹çš„ç»“æœ
                    model_results[model] = {
                        "target_answer": target_answer,
                        "sampled_answers": sampled_answers,
                        "hallucination_analysis": hallu_result
                    }
                
                # å¦‚æœæ²¡æœ‰ä»»ä½•æ¨¡å‹æˆåŠŸï¼Œè·³è¿‡è¯¥æ ·æœ¬
                if not model_results:
                    print(f"âŒ All models failed for sample {sample_id}")
                    continue
                
                # å°†å„æ¨¡å‹ç»“æœæ·»åŠ åˆ°å½“å‰æ ·æœ¬
                for model in model_results:
                    current_result[model] = model_results[model]
                
                # å®æ—¶ä¿å­˜å•ä¸ªæ ·æœ¬ç»“æœ
                single_save_path = os.path.join(SAVE_DIR, f"sample_{sample_id}_result.json")
                with open(single_save_path, "w", encoding="utf-8") as f:
                    # ç¡®ä¿æ‰€æœ‰æ•°æ®éƒ½å¯åºåˆ—åŒ–
                    serializable_result = current_result.copy()
                    json.dump(serializable_result, f, indent=4, ensure_ascii=False)
                
                # å¯è§†åŒ–å½“å‰æ ·æœ¬çš„æ¨¡å‹å¯¹æ¯”
                visualize_sample_results_comparison(model_results, sample_id, SAVE_DIR)
                
                # æ·»åŠ åˆ°æ€»ç»“æœåˆ—è¡¨
                all_results.append(current_result)
                processed_count += 1
                
                print(f"âœ… Sample {sample_id} processed and saved successfully")
                
            except Exception as e:
                print(f"âŒ Sample {sample_id} processing failed: {str(e)}")
                import traceback
                traceback.print_exc()
                continue
        
        # 3. ä¿å­˜å…¨éƒ¨ç»“æœ
        if all_results:
            # åˆ›å»ºå¯åºåˆ—åŒ–çš„å‰¯æœ¬
            serializable_all_results = []
            for result in all_results:
                serializable_result = result.copy()
                serializable_all_results.append(serializable_result)
            
            with open(os.path.join(SAVE_DIR, "all_results.json"), "w", encoding="utf-8") as f:
                json.dump(serializable_all_results, f, indent=4, ensure_ascii=False)
            
            # 4. åˆ›å»ºæ¨¡å‹å¯¹æ¯”æ±‡æ€»å¯è§†åŒ–
            create_comparison_summary_visualization(all_results, SAVE_DIR)
            
            print(f"\nğŸ‰ ã€Task Completedã€‘")
            print(f"âœ… Successfully processed: {processed_count}/{len(samples)} samples")
            print(f"ğŸ“ Results saved to: {os.path.abspath(SAVE_DIR)}")
            print(f"ğŸ“Š Generated charts: {processed_count} comparison charts + 1 summary chart")
            print(f"ğŸ” Models compared: {', '.join(MODELS)}")
        else:
            print("\nâš ï¸ No samples were successfully processed")
        
    except Exception as e:
        print(f"\nâŒ ã€Errorã€‘Process interrupted: {str(e)}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()