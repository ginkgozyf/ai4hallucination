import json
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import numpy as np
import os
from matplotlib import font_manager
import matplotlib.gridspec as gridspec

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

# åˆ›å»ºä¿å­˜ç»“æœçš„æ–‡ä»¶å¤¹
output_dir = "visualization_analysis"
os.makedirs(output_dir, exist_ok=True)

EXP_FILE_NAME = './experiment_file.json'


data = json.load(open(EXP_FILE_NAME))



def create_dataframe(data):
    """å°†æ•°æ®è½¬æ¢ä¸ºDataFrameæ ¼å¼ä¾¿äºåˆ†æ"""
    records = []
    
    for item in data['data']:
        question_id = item['question'][:30] + "..." if len(item['question']) > 30 else item['question']
        
        # ä¸ºæ¯ä¸ªæ¨¡å‹åˆ›å»ºè®°å½•
        for model in ['openai', 'rag', 'self_rag']:
            record = {
                'question': item['question'],
                'question_id': question_id,
                'model': model,
                'time': item[f'{model}_time'] if model != 'self_rag' else item['selfrag_time'],
                'context_recall': item[f'{model}_answer_ragas_evaluation']['context_recall'],
                'faithfulness': item[f'{model}_answer_ragas_evaluation']['faithfulness'],
                'factual_correctness': item[f'{model}_answer_ragas_evaluation']['factual_correctness(mode=f1)'],
                'hallucination_scores': item[f'{model}_answer_selfcheckgpt_sentence_hallucination_scores'],
                'answer': item[f'{model}_answer'],
                'reference_answer': item['answer']
            }
            records.append(record)
    
    return pd.DataFrame(records)

# åˆ›å»ºDataFrame
df = create_dataframe(data)

print("æ•°æ®æ¦‚è§ˆ:")
print(f"æ€»é—®é¢˜æ•°: {len(data['data'])}")
print(f"æ€»è®°å½•æ•°: {len(df)}")
print("\næ¨¡å‹åˆ†å¸ƒ:")
print(df['model'].value_counts())

# 1. å“åº”æ—¶é—´åˆ†æ
print("\n=== å“åº”æ—¶é—´åˆ†æ ===")
fig, axes = plt.subplots(2, 2, figsize=(15, 12))
fig.suptitle('æ¨¡å‹å“åº”æ—¶é—´åˆ†æ', fontsize=16, fontweight='bold')

# 1.1 å„æ¨¡å‹å“åº”æ—¶é—´æ¯”è¾ƒ
time_data = df.groupby('model')['time'].mean().sort_values(ascending=False)
axes[0,0].bar(time_data.index, time_data.values, color=['#ff6b6b', '#4ecdc4', '#45b7d1'])
axes[0,0].set_title('å„æ¨¡å‹å¹³å‡å“åº”æ—¶é—´')
axes[0,0].set_ylabel('æ—¶é—´ (ç§’)')
axes[0,0].tick_params(axis='x', rotation=45)
for i, v in enumerate(time_data.values):
    axes[0,0].text(i, v + 0.5, f'{v:.2f}s', ha='center', va='bottom')

# 1.2 å„é—®é¢˜å“åº”æ—¶é—´å¯¹æ¯”
time_by_question = df.pivot_table(index='question_id', columns='model', values='time')
time_by_question.plot(kind='bar', ax=axes[0,1], color=['#ff6b6b', '#4ecdc4', '#45b7d1'])
axes[0,1].set_title('å„é—®é¢˜ä¸åŒæ¨¡å‹å“åº”æ—¶é—´')
axes[0,1].set_ylabel('æ—¶é—´ (ç§’)')
axes[0,1].tick_params(axis='x', rotation=45)
axes[0,1].legend(title='æ¨¡å‹')

# 1.3 å“åº”æ—¶é—´åˆ†å¸ƒ
time_stats = df.groupby('model')['time'].agg(['mean', 'std', 'min', 'max'])
axes[1,0].barh(time_stats.index, time_stats['mean'], xerr=time_stats['std'], 
               color=['#ff6b6b', '#4ecdc4', '#45b7d1'], alpha=0.7)
axes[1,0].set_title('å“åº”æ—¶é—´ç»Ÿè®¡ (å‡å€¼Â±æ ‡å‡†å·®)')
axes[1,0].set_xlabel('æ—¶é—´ (ç§’)')

# 1.4 å“åº”æ—¶é—´çƒ­åŠ›å›¾
time_matrix = df.pivot_table(index='question_id', columns='model', values='time')
sns.heatmap(time_matrix, annot=True, fmt='.2f', cmap='YlOrRd', ax=axes[1,1])
axes[1,1].set_title('å“åº”æ—¶é—´çƒ­åŠ›å›¾ (ç§’)')

plt.tight_layout()
plt.savefig(f'{output_dir}/1_å“åº”æ—¶é—´åˆ†æ.png', dpi=300, bbox_inches='tight')
plt.close()

# 2. è¯„ä¼°æŒ‡æ ‡åˆ†æ
print("\n=== è¯„ä¼°æŒ‡æ ‡åˆ†æ ===")
fig, axes = plt.subplots(2, 3, figsize=(18, 12))
fig.suptitle('æ¨¡å‹è¯„ä¼°æŒ‡æ ‡åˆ†æ', fontsize=16, fontweight='bold')

metrics = ['context_recall', 'faithfulness', 'factual_correctness']
colors = ['#ff6b6b', '#4ecdc4', '#45b7d1']

for i, metric in enumerate(metrics):
    # å„æ¨¡å‹æŒ‡æ ‡æ¯”è¾ƒ
    metric_data = df.groupby('model')[metric].mean()
    axes[0,i].bar(metric_data.index, metric_data.values, color=colors)
    axes[0,i].set_title(f'{metric} æ¯”è¾ƒ')
    axes[0,i].set_ylabel('å¾—åˆ†')
    axes[0,i].set_ylim(0, 1)
    axes[0,i].tick_params(axis='x', rotation=45)
    for j, v in enumerate(metric_data.values):
        axes[0,i].text(j, v + 0.02, f'{v:.3f}', ha='center', va='bottom')
    
    # å„é—®é¢˜æŒ‡æ ‡å¯¹æ¯”
    metric_by_question = df.pivot_table(index='question_id', columns='model', values=metric)
    metric_by_question.plot(kind='bar', ax=axes[1,i], color=colors)
    axes[1,i].set_title(f'å„é—®é¢˜ {metric} å¯¹æ¯”')
    axes[1,i].set_ylabel('å¾—åˆ†')
    axes[1,i].set_ylim(0, 1)
    axes[1,i].tick_params(axis='x', rotation=45)
    axes[1,i].legend(title='æ¨¡å‹')

plt.tight_layout()
plt.savefig(f'{output_dir}/2_è¯„ä¼°æŒ‡æ ‡åˆ†æ.png', dpi=300, bbox_inches='tight')
plt.close()

# 3. å¹»è§‰åˆ†æ•°åˆ†æ
print("\n=== å¹»è§‰åˆ†æ•°åˆ†æ ===")
fig, axes = plt.subplots(2, 2, figsize=(15, 12))
fig.suptitle('æ¨¡å‹å¹»è§‰åˆ†æ•°åˆ†æ', fontsize=16, fontweight='bold')

# 3.1 å¹³å‡å¹»è§‰åˆ†æ•°
hallucination_means = []
for model in df['model'].unique():
    scores = []
    for hall_scores in df[df['model'] == model]['hallucination_scores']:
        scores.extend(hall_scores)
    hallucination_means.append(np.mean(scores) if scores else 0)

axes[0,0].bar(df['model'].unique(), hallucination_means, color=colors)
axes[0,0].set_title('å„æ¨¡å‹å¹³å‡å¹»è§‰åˆ†æ•°')
axes[0,0].set_ylabel('å¹»è§‰åˆ†æ•°')
axes[0,0].set_ylim(0, 1)
for i, v in enumerate(hallucination_means):
    axes[0,0].text(i, v + 0.02, f'{v:.3f}', ha='center', va='bottom')

# 3.2 å¹»è§‰åˆ†æ•°åˆ†å¸ƒ
all_scores = []
labels = []
for model in df['model'].unique():
    model_scores = []
    for hall_scores in df[df['model'] == model]['hallucination_scores']:
        model_scores.extend(hall_scores)
    all_scores.append(model_scores)
    labels.append(model)

axes[0,1].boxplot(all_scores, labels=labels)
axes[0,1].set_title('å¹»è§‰åˆ†æ•°åˆ†å¸ƒ')
axes[0,1].set_ylabel('å¹»è§‰åˆ†æ•°')

# 3.3 å„é—®é¢˜å¹»è§‰åˆ†æ•°å¯¹æ¯”
question_hallucination = []
for question in df['question_id'].unique():
    question_data = []
    for model in df['model'].unique():
        scores = []
        for idx, row in df[(df['question_id'] == question) & (df['model'] == model)].iterrows():
            scores.extend(row['hallucination_scores'])
        question_data.append(np.mean(scores) if scores else 0)
    question_hallucination.append(question_data)

x = np.arange(len(df['question_id'].unique()))
width = 0.25
for i, model in enumerate(df['model'].unique()):
    axes[1,0].bar(x + i*width, [q[i] for q in question_hallucination], width, label=model, color=colors[i])
axes[1,0].set_title('å„é—®é¢˜å¹»è§‰åˆ†æ•°å¯¹æ¯”')
axes[1,0].set_ylabel('å¹³å‡å¹»è§‰åˆ†æ•°')
axes[1,0].set_xticks(x + width)
axes[1,0].set_xticklabels(df['question_id'].unique(), rotation=45)
axes[1,0].legend()

# 3.4 é«˜å¹»è§‰åˆ†æ•°æ¯”ä¾‹
high_hallucination_ratio = []
for model in df['model'].unique():
    all_scores_flat = []
    for scores in df[df['model'] == model]['hallucination_scores']:
        all_scores_flat.extend(scores)
    high_ratio = len([s for s in all_scores_flat if s > 0.5]) / len(all_scores_flat) if all_scores_flat else 0
    high_hallucination_ratio.append(high_ratio)

axes[1,1].bar(df['model'].unique(), high_hallucination_ratio, color=colors)
axes[1,1].set_title('é«˜å¹»è§‰åˆ†æ•° (>0.5) æ¯”ä¾‹')
axes[1,1].set_ylabel('æ¯”ä¾‹')
axes[1,1].set_ylim(0, 1)
for i, v in enumerate(high_hallucination_ratio):
    axes[1,1].text(i, v + 0.02, f'{v:.1%}', ha='center', va='bottom')

plt.tight_layout()
plt.savefig(f'{output_dir}/3_å¹»è§‰åˆ†æ•°åˆ†æ.png', dpi=300, bbox_inches='tight')
plt.close()

# 4. ç»¼åˆæ€§èƒ½é›·è¾¾å›¾
print("\n=== ç»¼åˆæ€§èƒ½åˆ†æ ===")
fig = plt.figure(figsize=(12, 8))

# è®¡ç®—ç»¼åˆå¾—åˆ†ï¼ˆåè½¬æ—¶é—´å¾—åˆ†ï¼Œæ—¶é—´è¶ŠçŸ­å¾—åˆ†è¶Šé«˜ï¼‰
max_time = df['time'].max()
performance_data = []

for model in df['model'].unique():
    model_data = df[df['model'] == model]
    performance = {
        'model': model,
        'æ—¶é—´æ•ˆç‡': 1 - (model_data['time'].mean() / max_time),  # åè½¬æ—¶é—´å¾—åˆ†
        'ä¸Šä¸‹æ–‡å¬å›': model_data['context_recall'].mean(),
        'å¿ å®åº¦': model_data['faithfulness'].mean(),
        'äº‹å®æ­£ç¡®æ€§': model_data['factual_correctness'].mean(),
        'ä½å¹»è§‰æ€§': 1 - np.mean([score for scores in model_data['hallucination_scores'] for score in scores])  # åè½¬å¹»è§‰åˆ†æ•°
    }
    performance_data.append(performance)

# åˆ›å»ºé›·è¾¾å›¾
categories = list(performance_data[0].keys())[1:]
N = len(categories)

angles = [n / float(N) * 2 * np.pi for n in range(N)]
angles += angles[:1]

ax = plt.subplot(111, polar=True)
plt.xticks(angles[:-1], categories, color='grey', size=10)
ax.set_rlabel_position(0)
plt.yticks([0.2, 0.4, 0.6, 0.8], ["0.2", "0.4", "0.6", "0.8"], color="grey", size=8)
plt.ylim(0, 1)

for i, perf in enumerate(performance_data):
    values = list(perf.values())[1:]
    values += values[:1]
    ax.plot(angles, values, linewidth=2, linestyle='solid', label=perf['model'], color=colors[i])
    ax.fill(angles, values, alpha=0.1, color=colors[i])

plt.title('æ¨¡å‹ç»¼åˆæ€§èƒ½é›·è¾¾å›¾', size=14, y=1.08)
plt.legend(loc='upper right', bbox_to_anchor=(1.3, 1.0))
plt.savefig(f'{output_dir}/4_ç»¼åˆæ€§èƒ½é›·è¾¾å›¾.png', dpi=300, bbox_inches='tight')
plt.close()

# 5. ç›¸å…³æ€§åˆ†æ
print("\n=== ç›¸å…³æ€§åˆ†æ ===")
# åˆ›å»ºç›¸å…³æ€§æ•°æ®
correlation_data = []
for _, row in df.iterrows():
    avg_hallucination = np.mean(row['hallucination_scores']) if row['hallucination_scores'] else 0
    correlation_data.append({
        'time': row['time'],
        'context_recall': row['context_recall'],
        'faithfulness': row['faithfulness'],
        'factual_correctness': row['factual_correctness'],
        'hallucination': avg_hallucination
    })

corr_df = pd.DataFrame(correlation_data)

fig, axes = plt.subplots(2, 2, figsize=(15, 12))
fig.suptitle('æŒ‡æ ‡é—´ç›¸å…³æ€§åˆ†æ', fontsize=16, fontweight='bold')

# 5.1 æ—¶é—´ä¸è´¨é‡æŒ‡æ ‡çš„ç›¸å…³æ€§
axes[0,0].scatter(corr_df['time'], corr_df['factual_correctness'], alpha=0.7, s=100)
axes[0,0].set_xlabel('å“åº”æ—¶é—´ (ç§’)')
axes[0,0].set_ylabel('äº‹å®æ­£ç¡®æ€§')
axes[0,0].set_title('å“åº”æ—¶é—´ vs äº‹å®æ­£ç¡®æ€§')
correlation = corr_df['time'].corr(corr_df['factual_correctness'])
axes[0,0].text(0.05, 0.95, f'ç›¸å…³ç³»æ•°: {correlation:.3f}', transform=axes[0,0].transAxes, 
               bbox=dict(boxstyle="round,pad=0.3", facecolor="white", alpha=0.8))

# 5.2 å¿ å®åº¦ä¸äº‹å®æ­£ç¡®æ€§çš„ç›¸å…³æ€§
axes[0,1].scatter(corr_df['faithfulness'], corr_df['factual_correctness'], alpha=0.7, s=100)
axes[0,1].set_xlabel('å¿ å®åº¦')
axes[0,1].set_ylabel('äº‹å®æ­£ç¡®æ€§')
axes[0,1].set_title('å¿ å®åº¦ vs äº‹å®æ­£ç¡®æ€§')
correlation = corr_df['faithfulness'].corr(corr_df['factual_correctness'])
axes[0,1].text(0.05, 0.95, f'ç›¸å…³ç³»æ•°: {correlation:.3f}', transform=axes[0,1].transAxes,
               bbox=dict(boxstyle="round,pad=0.3", facecolor="white", alpha=0.8))

# 5.3 å¹»è§‰åˆ†æ•°ä¸äº‹å®æ­£ç¡®æ€§çš„ç›¸å…³æ€§
axes[1,0].scatter(corr_df['hallucination'], corr_df['factual_correctness'], alpha=0.7, s=100)
axes[1,0].set_xlabel('å¹»è§‰åˆ†æ•°')
axes[1,0].set_ylabel('äº‹å®æ­£ç¡®æ€§')
axes[1,0].set_title('å¹»è§‰åˆ†æ•° vs äº‹å®æ­£ç¡®æ€§')
correlation = corr_df['hallucination'].corr(corr_df['factual_correctness'])
axes[1,0].text(0.05, 0.95, f'ç›¸å…³ç³»æ•°: {correlation:.3f}', transform=axes[1,0].transAxes,
               bbox=dict(boxstyle="round,pad=0.3", facecolor="white", alpha=0.8))

# 5.4 æ•´ä½“ç›¸å…³æ€§çƒ­åŠ›å›¾
correlation_matrix = corr_df.corr()
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0, ax=axes[1,1])
axes[1,1].set_title('æŒ‡æ ‡é—´ç›¸å…³æ€§çƒ­åŠ›å›¾')

plt.tight_layout()
plt.savefig(f'{output_dir}/5_ç›¸å…³æ€§åˆ†æ.png', dpi=300, bbox_inches='tight')
plt.close()

# 6. è¯¦ç»†ç»Ÿè®¡è¡¨æ ¼
print("\n=== ç”Ÿæˆè¯¦ç»†ç»Ÿè®¡è¡¨æ ¼ ===")

# 6.1 åŸºæœ¬ç»Ÿè®¡è¡¨
basic_stats = df.groupby('model').agg({
    'time': ['mean', 'std', 'min', 'max'],
    'context_recall': 'mean',
    'faithfulness': 'mean', 
    'factual_correctness': 'mean'
}).round(3)

basic_stats.columns = ['æ—¶é—´å‡å€¼', 'æ—¶é—´æ ‡å‡†å·®', 'æœ€çŸ­æ—¶é—´', 'æœ€é•¿æ—¶é—´', 'ä¸Šä¸‹æ–‡å¬å›', 'å¿ å®åº¦', 'äº‹å®æ­£ç¡®æ€§']
basic_stats.to_csv(f'{output_dir}/åŸºæœ¬ç»Ÿè®¡è¡¨.csv', encoding='utf-8-sig')

# 6.2 å¹»è§‰ç»Ÿè®¡è¡¨
hallucination_stats = []
for model in df['model'].unique():
    model_data = df[df['model'] == model]
    all_scores = [score for scores in model_data['hallucination_scores'] for score in scores]
    if all_scores:
        stats = {
            'æ¨¡å‹': model,
            'å¹³å‡å¹»è§‰åˆ†æ•°': np.mean(all_scores),
            'å¹»è§‰åˆ†æ•°æ ‡å‡†å·®': np.std(all_scores),
            'æœ€å¤§å¹»è§‰åˆ†æ•°': np.max(all_scores),
            'é«˜å¹»è§‰æ¯”ä¾‹(>0.5)': len([s for s in all_scores if s > 0.5]) / len(all_scores),
            'æ€»å¥å­æ•°': len(all_scores)
        }
        hallucination_stats.append(stats)

hallucination_df = pd.DataFrame(hallucination_stats).round(3)
hallucination_df.to_csv(f'{output_dir}/å¹»è§‰ç»Ÿè®¡è¡¨.csv', encoding='utf-8-sig', index=False)

# 6.3 æ€§èƒ½æ’åè¡¨
performance_ranking = []
metrics_for_ranking = ['context_recall', 'faithfulness', 'factual_correctness', 'time']

for metric in metrics_for_ranking:
    if metric == 'time':
        # æ—¶é—´è¶ŠçŸ­è¶Šå¥½
        ranked = df.groupby('model')[metric].mean().sort_values(ascending=True)
    else:
        # å…¶ä»–æŒ‡æ ‡è¶Šé«˜è¶Šå¥½
        ranked = df.groupby('model')[metric].mean().sort_values(ascending=False)
    
    for i, (model, value) in enumerate(ranked.items()):
        performance_ranking.append({
            'æŒ‡æ ‡': metric,
            'æ¨¡å‹': model,
            'å¾—åˆ†': value,
            'æ’å': i + 1
        })

performance_ranking_df = pd.DataFrame(performance_ranking)
performance_ranking_df.to_csv(f'{output_dir}/æ€§èƒ½æ’åè¡¨.csv', encoding='utf-8-sig', index=False)

# 7. ç”Ÿæˆåˆ†ææŠ¥å‘Š
print("\n=== ç”Ÿæˆåˆ†ææŠ¥å‘Š ===")
report_content = f"""
# AIæ¨¡å‹æ€§èƒ½åˆ†ææŠ¥å‘Š

## æ•°æ®æ¦‚è§ˆ
- åˆ†æé—®é¢˜æ•°é‡: {len(data['data'])}
- è¯„ä¼°æ¨¡å‹æ•°é‡: {len(df['model'].unique())}
- æ€»æ•°æ®è®°å½•: {len(df)}

## ä¸»è¦å‘ç°

### 1. å“åº”æ—¶é—´è¡¨ç°
- æœ€å¿«æ¨¡å‹: {df.groupby('model')['time'].mean().idxmin()} (å¹³å‡ {df.groupby('model')['time'].mean().min():.2f}ç§’)
- æœ€æ…¢æ¨¡å‹: {df.groupby('model')['time'].mean().idxmax()} (å¹³å‡ {df.groupby('model')['time'].mean().max():.2f}ç§’)
- æ—¶é—´å·®å¼‚: {df.groupby('model')['time'].mean().max() / df.groupby('model')['time'].mean().min():.1f}å€

### 2. è´¨é‡æŒ‡æ ‡è¡¨ç°
- æœ€ä½³ä¸Šä¸‹æ–‡å¬å›: {df.groupby('model')['context_recall'].mean().idxmax()} ({df.groupby('model')['context_recall'].mean().max():.3f})
- æœ€ä½³å¿ å®åº¦: {df.groupby('model')['faithfulness'].mean().idxmax()} ({df.groupby('model')['faithfulness'].mean().max():.3f})
- æœ€ä½³äº‹å®æ­£ç¡®æ€§: {df.groupby('model')['factual_correctness'].mean().idxmax()} ({df.groupby('model')['factual_correctness'].mean().max():.3f})

### 3. å¹»è§‰æ§åˆ¶è¡¨ç°
- æœ€ä½å¹³å‡å¹»è§‰åˆ†æ•°: {hallucination_df.loc[hallucination_df['å¹³å‡å¹»è§‰åˆ†æ•°'].idxmin(), 'æ¨¡å‹']} ({hallucination_df['å¹³å‡å¹»è§‰åˆ†æ•°'].min():.3f})
- æœ€ä½³å¹»è§‰æ§åˆ¶: {hallucination_df.loc[hallucination_df['é«˜å¹»è§‰æ¯”ä¾‹(>0.5)'].idxmin(), 'æ¨¡å‹']} (é«˜å¹»è§‰æ¯”ä¾‹: {hallucination_df['é«˜å¹»è§‰æ¯”ä¾‹(>0.5)'].min():.1%})

## å»ºè®®
åŸºäºåˆ†æç»“æœï¼Œå»ºè®®åœ¨ä¸åŒåœºæ™¯ä¸‹é€‰æ‹©åˆé€‚çš„æ¨¡å‹ï¼š
- éœ€è¦å¿«é€Ÿå“åº”æ—¶: é€‰æ‹©å“åº”æ—¶é—´çŸ­çš„æ¨¡å‹
- éœ€è¦é«˜å‡†ç¡®æ€§æ—¶: é€‰æ‹©äº‹å®æ­£ç¡®æ€§é«˜çš„æ¨¡å‹  
- éœ€è¦å¯é ä¿¡æ¯æ—¶: é€‰æ‹©å¹»è§‰åˆ†æ•°ä½çš„æ¨¡å‹

ç”Ÿæˆæ—¶é—´: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}
"""

with open(f'{output_dir}/åˆ†ææŠ¥å‘Š.md', 'w', encoding='utf-8') as f:
    f.write(report_content)

# 8. ç”Ÿæˆæ±‡æ€»ä»ªè¡¨æ¿
print("\n=== ç”Ÿæˆæ±‡æ€»ä»ªè¡¨æ¿ ===")
fig = plt.figure(figsize=(20, 15))
gs = gridspec.GridSpec(3, 3, figure=fig)

# 8.1 æ€»ä½“æ€§èƒ½å¯¹æ¯”
ax1 = fig.add_subplot(gs[0, :])
overall_scores = []
models = df['model'].unique()
for model in models:
    model_data = df[df['model'] == model]
    # è®¡ç®—ç»¼åˆå¾—åˆ†ï¼ˆæ’é™¤æ—¶é—´ï¼Œå› ä¸ºé‡çº²ä¸åŒï¼‰
    avg_score = (model_data['context_recall'].mean() + 
                model_data['faithfulness'].mean() + 
                model_data['factual_correctness'].mean()) / 3
    overall_scores.append(avg_score)

bars = ax1.bar(models, overall_scores, color=colors, alpha=0.8)
ax1.set_title('æ¨¡å‹ç»¼åˆè´¨é‡å¾—åˆ†å¯¹æ¯”', fontsize=16, fontweight='bold')
ax1.set_ylabel('å¹³å‡å¾—åˆ†')
ax1.set_ylim(0, 1)
for bar, score in zip(bars, overall_scores):
    ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02, 
             f'{score:.3f}', ha='center', va='bottom', fontweight='bold')

# 8.2 æ—¶é—´-è´¨é‡æ•£ç‚¹å›¾
ax2 = fig.add_subplot(gs[1, 0])
for i, model in enumerate(models):
    model_data = df[df['model'] == model]
    avg_time = model_data['time'].mean()
    avg_quality = (model_data['context_recall'].mean() + 
                  model_data['faithfulness'].mean() + 
                  model_data['factual_correctness'].mean()) / 3
    ax2.scatter(avg_time, avg_quality, s=200, color=colors[i], label=model, alpha=0.7)
    ax2.annotate(model, (avg_time, avg_quality), xytext=(5, 5), textcoords='offset points')
ax2.set_xlabel('å¹³å‡å“åº”æ—¶é—´ (ç§’)')
ax2.set_ylabel('å¹³å‡è´¨é‡å¾—åˆ†')
ax2.set_title('æ—¶é—´-è´¨é‡å¹³è¡¡åˆ†æ')
ax2.grid(True, alpha=0.3)

# 8.3 å„æŒ‡æ ‡è¯¦ç»†å¯¹æ¯”
ax3 = fig.add_subplot(gs[1, 1:])
metrics_comparison = df.groupby('model')[['context_recall', 'faithfulness', 'factual_correctness']].mean()
x = np.arange(len(models))
width = 0.25
for i, metric in enumerate(['context_recall', 'faithfulness', 'factual_correctness']):
    ax3.bar(x + i*width, metrics_comparison[metric], width, 
            label=metric, alpha=0.8)
ax3.set_xlabel('æ¨¡å‹')
ax3.set_ylabel('å¾—åˆ†')
ax3.set_title('å„è´¨é‡æŒ‡æ ‡è¯¦ç»†å¯¹æ¯”')
ax3.set_xticks(x + width)
ax3.set_xticklabels(models)
ax3.legend()
ax3.set_ylim(0, 1)

# 8.4 å¹»è§‰æ§åˆ¶èƒ½åŠ›
ax4 = fig.add_subplot(gs[2, 0])
hallucination_bars = ax4.bar(hallucination_df['æ¨¡å‹'], hallucination_df['å¹³å‡å¹»è§‰åˆ†æ•°'], 
                            color=colors, alpha=0.8)
ax4.set_title('å¹³å‡å¹»è§‰åˆ†æ•°å¯¹æ¯”')
ax4.set_ylabel('å¹»è§‰åˆ†æ•°')
ax4.set_ylim(0, 1)
for bar, score in zip(hallucination_bars, hallucination_df['å¹³å‡å¹»è§‰åˆ†æ•°']):
    ax4.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02, 
             f'{score:.3f}', ha='center', va='bottom')

# 8.5 é«˜å¹»è§‰æ¯”ä¾‹
ax5 = fig.add_subplot(gs[2, 1])
high_hall_bars = ax5.bar(hallucination_df['æ¨¡å‹'], hallucination_df['é«˜å¹»è§‰æ¯”ä¾‹(>0.5)'], 
                        color=colors, alpha=0.8)
ax5.set_title('é«˜å¹»è§‰å¥å­æ¯”ä¾‹ (>0.5)')
ax5.set_ylabel('æ¯”ä¾‹')
ax5.set_ylim(0, 1)
for bar, ratio in zip(high_hall_bars, hallucination_df['é«˜å¹»è§‰æ¯”ä¾‹(>0.5)']):
    ax5.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02, 
             f'{ratio:.1%}', ha='center', va='bottom')

# 8.6 æ€§èƒ½æ€»ç»“
ax6 = fig.add_subplot(gs[2, 2])
ax6.axis('off')
summary_text = f"""
æ€§èƒ½æ€»ç»“:

ğŸ† ç»¼åˆæœ€ä½³: {max(zip(models, overall_scores), key=lambda x: x[1])[0]}
âš¡ æœ€å¿«å“åº”: {df.groupby('model')['time'].mean().idxmin()}
ğŸ¯ æœ€å‡†ç¡®: {df.groupby('model')['factual_correctness'].mean().idxmax()}
ğŸ” æœ€å¿ å®: {df.groupby('model')['faithfulness'].mean().idxmax()}
ğŸ’­ æœ€å°‘å¹»è§‰: {hallucination_df.loc[hallucination_df['å¹³å‡å¹»è§‰åˆ†æ•°'].idxmin(), 'æ¨¡å‹']}

å…³é”®å‘ç°:
â€¢ SelfRAGæ—¶é—´æ˜¾è‘—è¾ƒé•¿
â€¢ ä¸åŒæ¨¡å‹åœ¨ä¸åŒæŒ‡æ ‡å„æœ‰æ‰€é•¿
â€¢ éœ€è¦æƒè¡¡é€Ÿåº¦ä¸è´¨é‡
"""
ax6.text(0.1, 0.9, summary_text, transform=ax6.transAxes, fontsize=12, 
         verticalalignment='top', bbox=dict(boxstyle="round,pad=0.5", facecolor="lightblue", alpha=0.5))

plt.tight_layout()
plt.savefig(f'{output_dir}/8_æ±‡æ€»ä»ªè¡¨æ¿.png', dpi=300, bbox_inches='tight')
plt.close()

print(f"\n=== åˆ†æå®Œæˆ ===")
print(f"æ‰€æœ‰å›¾è¡¨å’Œè¡¨æ ¼å·²ä¿å­˜åˆ° '{output_dir}' æ–‡ä»¶å¤¹")
print(f"å…±ç”Ÿæˆæ–‡ä»¶:")
print(f"- 6ä¸ªåˆ†æå›¾è¡¨")
print(f"- 3ä¸ªç»Ÿè®¡è¡¨æ ¼") 
print(f"- 1ä»½åˆ†ææŠ¥å‘Š")
print(f"- 1ä¸ªæ±‡æ€»ä»ªè¡¨æ¿")

# æ˜¾ç¤ºå…³é”®ç»Ÿè®¡ç»“æœ
print(f"\nå…³é”®ç»Ÿè®¡ç»“æœ:")
print(f"å¹³å‡å“åº”æ—¶é—´: {df.groupby('model')['time'].mean().round(3).to_dict()}")
print(f"å¹³å‡äº‹å®æ­£ç¡®æ€§: {df.groupby('model')['factual_correctness'].mean().round(3).to_dict()}")
print(f"å¹³å‡å¹»è§‰åˆ†æ•°: {hallucination_df.set_index('æ¨¡å‹')['å¹³å‡å¹»è§‰åˆ†æ•°'].round(3).to_dict()}")